{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "**Task 1**"
      ],
      "metadata": {
        "id": "UsFhvnSOj23d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        return self.embedding(tokens)\n",
        "\n",
        "\n",
        "class PositionalEncoding(nn.Module):\n",
        "    def __init__(self, max_len, embed_size):\n",
        "        super(PositionalEncoding, self).__init__()\n",
        "        self.encoding = torch.zeros(max_len, embed_size)\n",
        "        for pos in range(max_len):\n",
        "            for i in range(0, embed_size, 2):\n",
        "                self.encoding[pos, i] = math.sin(pos / (10000 ** ((2 * i) / embed_size)))\n",
        "                self.encoding[pos, i + 1] = math.cos(pos / (10000 ** ((2 * (i + 1)) / embed_size)))\n",
        "        self.encoding = self.encoding.unsqueeze(0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return x + self.encoding[:, :x.size(1)].clone().detach()\n",
        "\n",
        "\n",
        "class GroupQueryAttention(nn.Module):\n",
        "    def __init__(self, embed_size, heads, num_groups=4):\n",
        "        super(GroupQueryAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.heads = heads\n",
        "        self.num_groups = num_groups\n",
        "        self.head_dim = embed_size // heads\n",
        "\n",
        "        assert (\n",
        "            self.head_dim * heads == embed_size\n",
        "        ), \"Embedding size should be divisible by heads\"\n",
        "\n",
        "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.queries = nn.Linear(self.head_dim, self.head_dim * self.num_groups, bias=False)\n",
        "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
        "\n",
        "    def forward(self, values, keys, query, mask):\n",
        "        N = query.shape[0]\n",
        "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
        "\n",
        "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
        "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
        "        queries = self.queries(query).reshape(N, query_len, self.heads, self.head_dim * self.num_groups)\n",
        "\n",
        "        values = self.values(values)\n",
        "        keys = self.keys(keys)\n",
        "\n",
        "        values = values.permute(0, 2, 1, 3)  # (N, heads, value_len, head_dim)\n",
        "        keys = keys.permute(0, 2, 3, 1)  # (N, heads, head_dim, key_len)\n",
        "        queries = queries.permute(0, 2, 1, 3)  # (N, heads, query_len, head_dim * num_groups)\n",
        "\n",
        "        energy = torch.matmul(queries, keys) / math.sqrt(self.head_dim)\n",
        "\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "\n",
        "        attention = torch.nn.functional.softmax(energy, dim=-1)\n",
        "\n",
        "        out = torch.matmul(attention, values)  # (N, heads, query_len, head_dim)\n",
        "        out = out.permute(0, 2, 1, 3).contiguous().reshape(N, query_len, self.heads * self.head_dim)\n",
        "        out = self.fc_out(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, embed_size, ff_hidden):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(embed_size, ff_hidden)\n",
        "        self.fc2 = nn.Linear(ff_hidden, embed_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.nn.functional.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "MMmEMCpIiTt4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class GPT2(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size=768, num_heads=12, num_layers=12, ff_hidden=3072, max_sequence_len=1024):\n",
        "        super(GPT2, self).__init__()\n",
        "\n",
        "        self.token_embedding = TokenEmbedding(vocab_size, embed_size)\n",
        "        self.positional_encoding = PositionalEncoding(max_sequence_len, embed_size)\n",
        "        self.transformer_blocks = nn.ModuleList(\n",
        "            [TransformerBlock(embed_size, num_heads, ff_hidden) for _ in range(num_layers)]\n",
        "        )\n",
        "        self.fc = nn.Linear(embed_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        tokens_embedding = self.token_embedding(x)\n",
        "        positional_encoded = self.positional_encoding(tokens_embedding)\n",
        "\n",
        "        for transformer in self.transformer_blocks:\n",
        "            positional_encoded = transformer(positional_encoded, mask)\n",
        "\n",
        "        output = self.fc(positional_encoded)\n",
        "        return output\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_size, heads, ff_hidden, dropout=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "\n",
        "        self.group_query_attention = GroupQueryAttention(embed_size, heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_size)\n",
        "        self.ff = FeedForward(embed_size, ff_hidden)\n",
        "        self.norm2 = nn.LayerNorm(embed_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        attention_output = self.group_query_attention(x, x, x, mask)\n",
        "        x = self.dropout(self.norm1(attention_output + x))\n",
        "        ff_output = self.ff(x)\n",
        "        x = self.dropout(self.norm2(ff_output + x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "H-9Cw3jFicgQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2\n",
        "# Including Rotary Positional Embedding and Group Query Attention (GQA)\n"
      ],
      "metadata": {
        "id": "RXXfYr1QkdpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        return self.embedding(tokens)\n",
        "\n",
        "class RotaryEmbedding(nn.Module):\n",
        "    def __init__(self, embed_size, max_sequence_len):\n",
        "        super(RotaryEmbedding, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.max_sequence_len = max_sequence_len\n",
        "        self.alpha = nn.Parameter(torch.zeros(self.embed_size // 2))\n",
        "        self.beta = nn.Parameter(torch.zeros(self.embed_size // 2))\n",
        "\n",
        "    def forward(self, x):\n",
        "        positions = torch.arange(0, x.shape[1], dtype=torch.float, device=x.device)\n",
        "        angles = positions.unsqueeze(1) / torch.pow(10000, 2 * torch.arange(0, self.embed_size, 2, dtype=torch.float, device=x.device) / self.embed_size)\n",
        "        angles = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1)\n",
        "\n",
        "        # Rotary transformation\n",
        "        sin, cos = angles[:, :, 0::2], angles[:, :, 1::2]\n",
        "        even = torch.sin(self.alpha * cos + self.beta * sin)\n",
        "        odd = torch.cos(self.alpha * cos + self.beta * sin)\n",
        "        angles = torch.cat([even, odd], dim=-1)\n",
        "\n",
        "        return x + angles.unsqueeze(0)\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    def __init__(self, embed_size, heads):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.heads = heads\n",
        "        self.head_dim = embed_size // heads\n",
        "\n",
        "        assert (\n",
        "            self.head_dim * heads == embed_size\n",
        "        ), \"Embedding size should be divisible by heads\"\n",
        "\n",
        "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
        "\n",
        "    def forward(self, values, keys, query, mask):\n",
        "        N = query.shape[0]\n",
        "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
        "\n",
        "        # Split the embedding into self.heads different pieces\n",
        "        values = values.reshape(N, value_len, self.heads, self.head_dim)\n",
        "        keys = keys.reshape(N, key_len, self.heads, self.head_dim)\n",
        "        queries = query.reshape(N, query_len, self.heads, self.head_dim)\n",
        "\n",
        "        values = self.values(values)\n",
        "        keys = self.keys(keys)\n",
        "        queries = self.queries(queries)\n",
        "\n",
        "        energy = torch.einsum(\"nqhd,nkhd->nhqk\", [queries, keys]) / math.sqrt(self.head_dim)\n",
        "\n",
        "        if mask is not None:\n",
        "            energy = energy.masked_fill(mask == 0, float(\"-1e20\"))\n",
        "\n",
        "        attention = torch.nn.functional.softmax(energy, dim=3)\n",
        "\n",
        "        out = torch.einsum(\"nhql,nlhd->nqhd\", [attention, values]).reshape(\n",
        "            N, query_len, self.heads * self.head_dim\n",
        "        )\n",
        "\n",
        "        out = self.fc_out(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, embed_size, ff_hidden):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(embed_size, ff_hidden)\n",
        "        self.fc2 = nn.Linear(ff_hidden, embed_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.nn.functional.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "7C_OHC6kic6F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_size, heads, ff_hidden, dropout=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "\n",
        "        self.attention = SelfAttention(embed_size, heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_size)\n",
        "        self.ff = FeedForward(embed_size, ff_hidden)\n",
        "        self.norm2 = nn.LayerNorm(embed_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        attention_output = self.attention(x, x, x, mask)\n",
        "        x = self.dropout(self.norm1(attention_output + x))\n",
        "        ff_output = self.ff(x)\n",
        "        x = self.dropout(self.norm2(ff_output + x))\n",
        "        return x\n",
        "\n",
        "class GPT2(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size=768, num_heads=12, num_layers=12, ff_hidden=3072, max_sequence_len=1024):\n",
        "        super(GPT2, self).__init__()\n",
        "\n",
        "        self.token_embedding = TokenEmbedding(vocab_size, embed_size)\n",
        "        self.rotary_embedding = RotaryEmbedding(embed_size, max_sequence_len)\n",
        "        self.transformer_blocks = nn.ModuleList(\n",
        "            [TransformerBlock(embed_size, num_heads, ff_hidden) for _ in range(num_layers)]\n",
        "        )\n",
        "        self.fc = nn.Linear(embed_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        tokens_embedding = self.token_embedding(x)\n",
        "        positional_encoded = self.rotary_embedding(tokens_embedding)\n",
        "\n",
        "        for transformer in self.transformer_blocks:\n",
        "            positional_encoded = transformer(positional_encoded, mask)\n",
        "\n",
        "        output = self.fc(positional_encoded)\n",
        "        return output"
      ],
      "metadata": {
        "id": "snkS-eBfkpsZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_size, heads, ff_hidden, dropout=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "\n",
        "        self.attention = SelfAttention(embed_size, heads)\n",
        "        self.norm1 = nn.LayerNorm(embed_size)\n",
        "        self.ff = FeedForward(embed_size, ff_hidden)\n",
        "        self.norm2 = nn.LayerNorm(embed_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        attention_output = self.attention(x, x, x, mask)\n",
        "        x = self.dropout(self.norm1(attention_output + x))\n",
        "        ff_output = self.ff(x)\n",
        "        x = self.dropout(self.norm2(ff_output + x))\n",
        "        return x\n",
        "\n",
        "class GPT2(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size=768, num_heads=12, num_layers=12, ff_hidden=3072, max_sequence_len=1024):\n",
        "        super(GPT2, self).__init__()\n",
        "\n",
        "        self.token_embedding = TokenEmbedding(vocab_size, embed_size)\n",
        "        self.rotary_embedding = RotaryEmbedding(embed_size, max_sequence_len)\n",
        "        self.transformer_blocks = nn.ModuleList(\n",
        "            [TransformerBlock(embed_size, num_heads, ff_hidden) for _ in range(num_layers)]\n",
        "        )\n",
        "        self.fc = nn.Linear(embed_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        tokens_embedding = self.token_embedding(x)\n",
        "        positional_encoded = self.rotary_embedding(tokens_embedding)\n",
        "\n",
        "        for transformer in self.transformer_blocks:\n",
        "            positional_encoded = transformer(positional_encoded, mask)\n",
        "\n",
        "        output = self.fc(positional_encoded)\n",
        "        return output"
      ],
      "metadata": {
        "id": "PQLuSBbjku4l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Task 2.3"
      ],
      "metadata": {
        "id": "BTAEUMhSOr0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TokenEmbedding(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size):\n",
        "        super(TokenEmbedding, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "\n",
        "    def forward(self, tokens):\n",
        "        return self.embedding(tokens)\n",
        "\n",
        "class RotaryEmbedding(nn.Module):\n",
        "    def __init__(self, embed_size, max_sequence_len):\n",
        "        super(RotaryEmbedding, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.max_sequence_len = max_sequence_len\n",
        "        self.alpha = nn.Parameter(torch.zeros(self.embed_size // 2))\n",
        "        self.beta = nn.Parameter(torch.zeros(self.embed_size // 2))\n",
        "\n",
        "    def forward(self, x):\n",
        "        positions = torch.arange(0, x.shape[1], dtype=torch.float, device=x.device)\n",
        "        angles = positions.unsqueeze(1) / torch.pow(10000, 2 * torch.arange(0, self.embed_size, 2, dtype=torch.float, device=x.device) / self.embed_size)\n",
        "        angles = torch.cat([torch.sin(angles), torch.cos(angles)], dim=-1)\n",
        "\n",
        "        # Rotary transformation\n",
        "        sin, cos = angles[:, :, 0::2], angles[:, :, 1::2]\n",
        "        even = torch.sin(self.alpha * cos + self.beta * sin)\n",
        "        odd = torch.cos(self.alpha * cos + self.beta * sin)\n",
        "        angles = torch.cat([even, odd], dim=-1)\n",
        "\n",
        "        return x + angles.unsqueeze(0)\n",
        "\n",
        "class SlidingWindowAttention(nn.Module):\n",
        "    def __init__(self, embed_size, heads, window_size=128, dropout=0.1):\n",
        "        super(SlidingWindowAttention, self).__init__()\n",
        "        self.embed_size = embed_size\n",
        "        self.heads = heads\n",
        "        self.window_size = window_size\n",
        "        self.head_dim = embed_size // heads\n",
        "        self.scale = self.head_dim ** -0.5\n",
        "\n",
        "        assert (\n",
        "            self.head_dim * heads == embed_size\n",
        "        ), \"Embedding size should be divisible by heads\"\n",
        "\n",
        "        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)\n",
        "        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, values, keys, query, mask):\n",
        "        N = query.shape[0]\n",
        "        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]\n",
        "\n",
        "        values = values.reshape(N, -1, self.heads, self.head_dim)\n",
        "        keys = keys.reshape(N, -1, self.heads, self.head_dim)\n",
        "        queries = self.queries(query).reshape(N, -1, self.heads, self.head_dim)\n",
        "\n",
        "        values = self.values(values)\n",
        "        keys = self.keys(keys)\n",
        "\n",
        "        output = torch.zeros(N, query_len, self.heads * self.head_dim, device=query.device)\n",
        "\n",
        "        for i in range(0, key_len, self.window_size):\n",
        "            start = i\n",
        "            end = min(start + self.window_size, key_len)\n",
        "\n",
        "            window_values = values[:, start:end]\n",
        "            window_keys = keys[:, start:end]\n",
        "\n",
        "            energy = torch.einsum(\"nqhd,nkhd->nhqk\", queries, window_keys) * self.scale\n",
        "\n",
        "            if mask is not None:\n",
        "                energy = energy.masked_fill(mask[:, :, start:end] == 0, float(\"-1e20\"))\n",
        "\n",
        "            attention = torch.nn.functional.softmax(energy, dim=-1)\n",
        "            out = torch.einsum(\"nhql,nlhd->nqhd\", attention, window_values).reshape(\n",
        "                N, -1, self.heads * self.head_dim\n",
        "            )\n",
        "            output[:, :, start * self.head_dim:end * self.head_dim] += out\n",
        "\n",
        "        output = self.fc_out(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, embed_size, ff_hidden):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.fc1 = nn.Linear(embed_size, ff_hidden)\n",
        "        self.fc2 = nn.Linear(ff_hidden, embed_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = torch.nn.functional.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "vd1R3HJCOeQO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, embed_size, heads, ff_hidden, window_size=128, dropout=0.1):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "\n",
        "        self.sliding_window_attention = SlidingWindowAttention(embed_size, heads, window_size, dropout)\n",
        "        self.norm1 = nn.LayerNorm(embed_size)\n",
        "        self.ff = FeedForward(embed_size, ff_hidden)\n",
        "        self.norm2 = nn.LayerNorm(embed_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        attention_output = self.sliding_window_attention(x, x, x, mask)\n",
        "        x = self.dropout(self.norm1(attention_output + x))\n",
        "        ff_output = self.ff(x)\n",
        "        x = self.dropout(self.norm2(ff_output + x))\n",
        "        return x\n",
        "\n",
        "class GPT2(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size=768, num_heads=12, num_layers=12, ff_hidden=3072, max_sequence_len=1024):\n",
        "        super(GPT2, self).__init__()\n",
        "\n",
        "        self.token_embedding = TokenEmbedding(vocab_size, embed_size)\n",
        "        self.rotary_embedding = RotaryEmbedding(embed_size, max_sequence_len)\n",
        "        self.transformer_blocks = nn.ModuleList(\n",
        "            [TransformerBlock(embed_size, num_heads, ff_hidden) for _ in range(num_layers)]\n",
        "        )\n",
        "        self.fc = nn.Linear(embed_size, vocab_size)\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        tokens_embedding = self.token_embedding(x)\n",
        "        positional_encoded = self.rotary_embedding(tokens_embedding)\n",
        "\n",
        "        for transformer in self.transformer_blocks:\n",
        "            positional_encoded = transformer(positional_encoded, mask)\n",
        "\n",
        "        output = self.fc(positional_encoded)\n",
        "        return output"
      ],
      "metadata": {
        "id": "YmUBBfbuOwUs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E9XFdIIHO0qB"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}